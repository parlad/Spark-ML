{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "readMultipleFile.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJLTFuP+5Jgp2Hq2Ilxki8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parlad/Spark-ML/blob/master/readMultipleFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PmmjiTJWfEi"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZZ0SDy2XBay"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k9anwRWXHlY"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGtUWt5WiEK"
      },
      "source": [
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext.getOrCreate();\n",
        "\n",
        "\n",
        " \n",
        "  # read input text files present in the directory to RDD\n",
        "  lines = sc.textFile(\"/content/file1.txt,/content/file2.txt,/content/file3.txt\")\n",
        " \n",
        "  # collect the RDD to a list\n",
        "  llist = lines.collect()\n",
        " \n",
        "  # print the list\n",
        "  for line in llist:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UTz567SZuJE"
      },
      "source": [
        "## Read all text files in a directory to single RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEk3LkPkWirp"
      },
      "source": [
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext.getOrCreate();\n",
        " \n",
        "  # read input text files present in the directory to RDD\n",
        "  lines = sc.textFile(\"/content/mldata/\")\n",
        " \n",
        "  # collect the RDD to a list\n",
        "  llist = lines.collect()\n",
        " \n",
        "  # print the list\n",
        "  for line in llist:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durgX9BYacKB"
      },
      "source": [
        "## Read all text files in multiple directories to single RDD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7gj7jPraKfs"
      },
      "source": [
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext.getOrCreate();\n",
        " \n",
        "  # read input text files present in the directory to RDD\n",
        "  lines = sc.textFile(\"/content/ml1data,/content/mldata\")\n",
        " \n",
        "  # collect the RDD to a list\n",
        "  llist = lines.collect()\n",
        " \n",
        "  # print the list\n",
        "  for line in llist:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei93tRaJa_CM"
      },
      "source": [
        "## Read all text files, matching a pattern, to single RDD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QViFXG1jb7Gt"
      },
      "source": [
        "## PySpark â€“ Split dataframe into equal number of rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzWGQKPvbK5N"
      },
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "\n",
        "# importing sparksession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving an app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# Column names for the dataframe\n",
        "columns = [\"Brand\", \"Product\"]\n",
        "\n",
        "# Row data for the dataframe\n",
        "data = [\n",
        "\t(\"HP\", \"Laptop\"),\n",
        "\t(\"Lenovo\", \"Mouse\"),\n",
        "\t(\"Dell\", \"Keyboard\"),\n",
        "\t(\"Samsung\", \"Monitor\"),\n",
        "\t(\"MSI\", \"Graphics Card\"),\n",
        "\t(\"Asus\", \"Motherboard\"),\n",
        "\t(\"Gigabyte\", \"Motherboard\"),\n",
        "\t(\"Zebronics\", \"Cabinet\"),\n",
        "\t(\"Adata\", \"RAM\"),\n",
        "\t(\"Transcend\", \"SSD\"),\n",
        "\t(\"Kingston\", \"HDD\"),\n",
        "\t(\"Toshiba\", \"DVD Writer\")\n",
        "]\n",
        "\n",
        "# Create the dataframe using the above values\n",
        "prod_df = spark.createDataFrame(data=data,\n",
        "\t\t\t\t\t\t\t\tschema=columns)\n",
        "\n",
        "# View the dataframe\n",
        "prod_df.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JYT0mmco5K"
      },
      "source": [
        "## Split the dataframe, perform the operation and concatenate the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDXpSuT2b9DL"
      },
      "source": [
        "# Define the number of splits you want\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.sql.functions import concat, col, lit\n",
        "\n",
        "n_splits = 4\n",
        "\n",
        "# Calculate count of each dataframe rows\n",
        "each_len = prod_df.count() // n_splits\n",
        "\n",
        "# Create a copy of original dataframe\n",
        "copy_df = prod_df\n",
        "\n",
        "# Function to modify columns of each individual split\n",
        "\n",
        "\n",
        "def modify_dataframe(data):\n",
        "\treturn data.select(\n",
        "\t\tconcat(col(\"Brand\"), lit(\" - \"),\n",
        "\t\t\tcol(\"Product\"))\n",
        "\t)\n",
        "\n",
        "\n",
        "# Create an empty dataframe to\n",
        "# store concatenated results\n",
        "schema = StructType([\n",
        "\tStructField('Brand - Product', StringType(), True)\n",
        "])\n",
        "result_df = spark.createDataFrame(data=[],\n",
        "\t\t\t\t\t\t\t\tschema=schema)\n",
        "\n",
        "# Iterate for each dataframe\n",
        "i = 0\n",
        "while i < n_splits:\n",
        "\n",
        "\t# Get the top `each_len` number of rows\n",
        "\ttemp_df = copy_df.limit(each_len)\n",
        "\n",
        "\t# Truncate the `copy_df` to remove\n",
        "\t# the contents fetched for `temp_df`\n",
        "\tcopy_df = copy_df.subtract(temp_df)\n",
        "\n",
        "\t# Perform operation on the newly created dataframe\n",
        "\ttemp_df_mod = modify_dataframe(data=temp_df)\n",
        "\ttemp_df_mod.show(truncate=False)\n",
        "\n",
        "\t# Concat the dataframe\n",
        "\tresult_df = result_df.union(temp_df_mod)\n",
        "\n",
        "\t# Increment the split number\n",
        "\ti += 1\n",
        "\n",
        "result_df.show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kncVQ5CKcX9Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SuImoyodSM4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0DWvsEGdSgU"
      },
      "source": [
        "## Other example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuwPswx6fZFX"
      },
      "source": [
        "!pip install kafka"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzAzG6dvfwhE"
      },
      "source": [
        "!pip install py4j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5gb875Of54V"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh3YGjcUdWyf"
      },
      "source": [
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# pip install kafka-python\n",
        "\n",
        "KAFKA_TOPIC_NAME_CONS = \"Topic\"\n",
        "KAFKA_BOOTSTRAP_SERVERS_CONS = 'localhost:9092'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Kafka Producer Application Started ... \")\n",
        "\n",
        "    kafka_producer_obj = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS_CONS,\n",
        "                                       value_serializer=lambda x: x.encode('utf-8'))\n",
        "    \n",
        "    filepath = \"IRIS.csv\"\n",
        "    \n",
        "    flower_df = pd.read_csv(filepath)\n",
        "  \n",
        "    flower_df['order_id'] = np.arange(len(flower_df))\n",
        "\n",
        "    \n",
        "    flower_list = flower_df.to_dict(orient=\"records\")\n",
        "       \n",
        "\n",
        "    message_list = []\n",
        "    message = None\n",
        "    for message in flower_list:\n",
        "        \n",
        "        message_fields_value_list = []\n",
        "               \n",
        "        message_fields_value_list.append(message[\"order_id\"])\n",
        "        message_fields_value_list.append(message[\"sepal_length\"])\n",
        "        message_fields_value_list.append(message[\"sepal_width\"])\n",
        "        message_fields_value_list.append(message[\"petal_length\"])\n",
        "        message_fields_value_list.append(message[\"petal_width\"])\n",
        "        message_fields_value_list.append(message[\"species\"])\n",
        "\n",
        "        message = ','.join(str(v) for v in message_fields_value_list)\n",
        "        print(\"Message Type: \", type(message))\n",
        "        print(\"Message: \", message)\n",
        "        kafka_producer_obj.send(KAFKA_TOPIC_NAME_CONS, message)\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "    print(\"Kafka Producer Application Completed. \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D1oQ2Kbemvh"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "kafka_topic_name = \"Topic\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Structured Streaming \") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Construct a streaming DataFrame that reads from topic\n",
        "flower_df = spark \\\n",
        "        .readStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
        "        .option(\"subscribe\", kafka_topic_name) \\\n",
        "        .option(\"startingOffsets\", \"latest\") \\\n",
        "        .load()\n",
        "\n",
        "flower_df1 = flower_df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
        "\n",
        "\n",
        "flower_schema_string = \"order_id INT,sepal_length DOUBLE,sepal_length DOUBLE,sepal_length DOUBLE,sepal_length DOUBLE,species STRING\"\n",
        "\n",
        "\n",
        "\n",
        "flower_df2 = flower_df1 \\\n",
        "        .select(from_csv(col(\"value\"), flower_schema_string) \\\n",
        "                .alias(\"flower\"), \"timestamp\")\n",
        "\n",
        "\n",
        "flower_df3 = flower_df2.select(\"flower.*\", \"timestamp\")\n",
        "\n",
        "    \n",
        "flower_df3.createOrReplaceTempView(\"flower_find\");\n",
        "song_find_text = spark.sql(\"SELECT * FROM flower_find\")\n",
        "flower_agg_write_stream = song_find_text \\\n",
        "        .writeStream \\\n",
        "        .trigger(processingTime='5 seconds') \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .option(\"truncate\", \"false\") \\\n",
        "        .format(\"memory\") \\\n",
        "        .queryName(\"testedTable\") \\\n",
        "        .start()\n",
        "\n",
        "flower_agg_write_stream.awaitTermination(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zxsUUkDgmoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}